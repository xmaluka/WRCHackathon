{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "        \n",
    "import pandas as pd     \n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import re\n",
    "from IPython.display import display\n",
    "import pyodbc\n",
    "# ---------\n",
    "from tweepy import OAuthHandler\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys\n",
    "\n",
    "CONSUMER_KEY    = '9qH6416zLY2GENYQzvkeFyP2o'\n",
    "CONSUMER_SECRET = '8XOo0ENIQy4HjX5jXC1xSdfj7jiOqubesglT29QsKkmHaDtEeh'\n",
    "\n",
    "ACCESS_TOKEN  = '1219722321289777164-hlhI8xizj3M39DU6VdVhsPyQuAWbgF'\n",
    "ACCESS_SECRET = 'l2WyzCqaaUfadV9P4N53zjJ3OOvqUb0IxNskk2GyOAiMM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authentication\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = [ 'leak' , 'burst' , 'pressure', 'water','cut', 'pipe',\n",
    "              'fault', 'broke', 'damage', 'toilet', 'sanit',\n",
    "              'waste', 'complain', 'supply','service','deliver',\n",
    "              'out', 'no', 'reference','report' ,'complain',\n",
    "              'install','low','repair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetCleaner(tweet):\n",
    "    tweet = str(tweet.lower()) \n",
    "    tweet = re.sub('@[\\w]*','',tweet) \n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "#     tweet = re.sub(r'#\\w*', '', tweet) might put issue in hashtag\n",
    "    tweet = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", ' ', tweet)\n",
    "    tweet = re.sub(r\"U+FFFD \", ' ', tweet)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    tweet = tweet.lstrip(' ')\n",
    "    tweet = ' '.join([i for i in tweet.split() if any(list_word in i for list_word in key_words)])\n",
    "    if len(tweet)==0:\n",
    "        tweet = np.nan\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraptweets(districts, date_since, numTweets, numRuns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'user_location',\n",
    "                                        'geo_location','message','keywords',\n",
    "                                        'hashtags','tweet_id','timestamp'])\n",
    "    tweet_list = []\n",
    "    for i in range(0, numRuns):\n",
    "        for district in districts:\n",
    "            tweets = tweepy.Cursor(api.search, q=district, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
    "            district_list = [tweet for tweet in tweets]\n",
    "            tweet_list += district_list\n",
    "# Begin scraping the tweets individually:\n",
    "    noTweets = 0\n",
    "    for tweet in tweet_list:\n",
    "        username = tweet.user.screen_name\n",
    "        if tweet.user.location:\n",
    "            user_location = tweet.user.location\n",
    "        else:\n",
    "            user_location = np.nan\n",
    "        if tweet.place:\n",
    "            geo_location = tweet.place.full_name\n",
    "        else:\n",
    "            geo_location = np.nan\n",
    "        hashtags = ','.join([i['text'] for i in tweet.entities['hashtags']])\n",
    "        if len(hashtags)==0:\n",
    "            hashtags=np.nan\n",
    "        try:\n",
    "            text = tweet.retweeted_status.full_text\n",
    "        except AttributeError:  # Not a Retweet\n",
    "            text = tweet.full_text\n",
    "        keywords = TweetCleaner(text)\n",
    "        tweet_id = tweet.id_str\n",
    "        timestamp = tweet.created_at\n",
    "        ith_tweet = [username,user_location,geo_location, text,keywords,hashtags,tweet_id,timestamp]\n",
    "# Append to dataframe - db_tweets\n",
    "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "# increase counter - noTweets  \n",
    "        noTweets += 1\n",
    "#     db_tweets.replace(np.nan,'',inplace=True)\n",
    "    return(db_tweets)\n",
    "#     time.sleep(920) #15 minute sleep time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# district_user_names = ['@alfrednzoDM','@AmajubaDM','@OfficialBCMM',\n",
    "#                         '@CWDM2','@EhlanzeniM','@FezileDabiDM',\n",
    "#                         '@GardenRoute_DM','@GertSibandeDM','@DistrictIlembe',\n",
    "#                         '@Cetshwayo_dm','@LejweleputswaM','@MangaungCity',\n",
    "#                         '@NkangalaDM','@SedibengDM','@UguDM','@UmkhanyakudeM',\n",
    "#                         '@OkhahlambaLM','@VhembeDM','@RandLocal','@EmfuleniLM']\n",
    "district_user_names = ['@thameswater']\n",
    "date_since = date.today() - timedelta(days=2) #Searches up to past 2 days\n",
    "numTweets = 100\n",
    "numRuns = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = scraptweets(district_user_names, date_since, numTweets, numRuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_DB(input_df):\n",
    "    df = input_df.copy()\n",
    "    from sqlalchemy import create_engine\n",
    "    SERVER = 'LAPTOP-1SDCE2AE\\SQLEXPRESS'\n",
    "    DATABASE = 'WRC_DB'\n",
    "    DRIVER = 'SQL Server Native Client 11.0'\n",
    "    DATABASE_CONN = f'mssql://{SERVER}/{DATABASE}?trusted_connection=yes&driver={DRIVER}'\n",
    "    engine = create_engine(DATABASE_CONN)\n",
    "    connection = engine.connect()\n",
    "    # Initialize large table if it doesnt exist:\n",
    "    if not engine.dialect.has_table(engine, 'DB_Large'):\n",
    "        df.to_sql('DB_Large',con=engine,index=False,chunksize=10000)\n",
    "    # DEFINE FUNCTION TO FILTER AND CLASSIFY ISSUES\n",
    "    # ALGORITHM TO INPUT SMALL TABLE HERE.. AS DF HAS TO BE FILTERED BY MODEL\n",
    "    old_data = pd.read_sql_query('SELECT tweet_id FROM DB_Large',connection).tweet_id.tolist()\n",
    "    to_drop = []\n",
    "    for i,row in df.iterrows():\n",
    "        if row['tweet_id'] in old_data:\n",
    "            to_drop.append(i)\n",
    "    df = df.drop(df.iloc[to_drop].index,axis=0)\n",
    "    df.to_sql('DB_Large',con=engine,if_exists='append',index=False,chunksize=10000)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_DB(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
